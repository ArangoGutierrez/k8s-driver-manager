#! /bin/bash
# Copyright (c) 2019-2021, NVIDIA CORPORATION.  All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

NODE_NAME=${NODE_NAME:?"Missing node name"}
DRAIN_USE_FORCE=${DRAIN_USE_FORCE:-"false"}
DRAIN_POD_SELECTOR_LABEL=${DRAIN_POD_SELECTOR_LABEL:-""}
DRAIN_TIMEOUT_SECONDS=${DRAIN_TIMEOUT_SECONDS:-"0s"}
DRAIN_DELETE_EMPTYDIR_DATA=${DRAIN_DELETE_EMPTYDIR_DATA:-"false"}
ENABLE_AUTO_DRAIN=${ENABLE_AUTO_DRAIN:-"true"}
DRIVER_PID_FILE=/run/nvidia/nvidia-driver.pid
OPERATOR_NAMESPACE=${OPERATOR_NAMESPACE:-"gpu-operator-resources"}
PAUSED_STR="paused-for-driver-upgrade"
PLUGIN_DEPLOYED=""
GFD_DEPLOYED=""
DCGM_DEPLOYED=""
DCGM_EXPORTER_DEPLOYED=""
TOOLKIT_DEPLOYED=""
VALIDATOR_DEPLOYED=""
MIG_MANAGER_DEPLOYED=""

_drain_k8s_node() {
    echo "Draining node ${NODE_NAME}..."
    kubectl drain ${NODE_NAME} --ignore-daemonsets=true --force=${DRAIN_USE_FORCE} --pod-selector=${DRAIN_POD_SELECTOR_LABEL} --delete-emptydir-data=${DRAIN_DELETE_EMPTYDIR_DATA} --timeout=${DRAIN_TIMEOUT_SECONDS}
    if [ "$?" != "0" ]; then
        return 1
    fi
    return 0
}

_is_auto_drain_enabled() {
    if [ "${ENABLE_AUTO_DRAIN}" = "true" ]; then
        return 0
    fi
    echo "Auto drain of the node ${NODE_NAME} is disabled"
    return 1
}

# Only return 'paused-*' if the value passed in is != 'false'. It should only
# be 'false' if some external entity has forced it to this value, at which point
# we want to honor it's existing value and not change it.
_maybe_set_paused() {
    local current_value="${1}"
    if [  "${current_value}" = "" ]; then
        # disabled by user with empty value, retain it
        echo ""
    elif [  "${current_value}" = "false" ]; then
        # disabled by user
        echo "false"
    elif [  "${current_value}" = "true" ]; then
        # disable
        echo "${PAUSED_STR}"
    elif [ "${current_value}" == *"${PAUSED_STR}"* ]; then
        # already added paused status for driver upgrade
        echo "${current_value}"
    else
        # append paused status for driver upgrade
        echo "${current_value}_${PAUSED_STR}"
    fi
}

# Only return 'true' if the value passed in is != 'false'. It should only
# be 'false' if some external entity has forced it to this value, at which point
# we want to honor it's existing value and not change it.
_maybe_set_true() {
    local current_value="${1}"
    if [  "${current_value}" = "false" ]; then
        # disabled by user
        echo "false"
    elif [  "${current_value}" = "${PAUSED_STR}" ]; then
        # enable the component
        echo "true"
    else
        # revert back to original label
        echo "${current_value}" | sed -r "s/${PAUSED_STR}//g" | tr -d "_"
    fi
}

_fetch_current_labels() {
    echo "Getting current value of the 'nvidia.com/gpu.deploy.operator-validator' node label"
    VALIDATOR_DEPLOYED=$(kubectl get nodes ${NODE_NAME} -o=jsonpath='{$.metadata.labels.nvidia\.com/gpu\.deploy\.operator-validator}')
    if [ "${?}" != "0" ]; then
        echo "Unable to get the value of the 'nvidia.com/gpu.deploy.operator-validator' label"
        exit 1
    fi
    echo "Current value of 'nvidia.com/gpu.deploy.operator-validator=${VALIDATOR_DEPLOYED}'"

    echo "Getting current value of the 'nvidia.com/gpu.deploy.container-toolkit' node label"
    TOOLKIT_DEPLOYED=$(kubectl get nodes ${NODE_NAME} -o=jsonpath='{$.metadata.labels.nvidia\.com/gpu\.deploy\.container-toolkit}')
    if [ "${?}" != "0" ]; then
        echo "Unable to get the value of the 'nvidia.com/gpu.deploy.container-toolkit' label"
        exit 1
    fi
    echo "Current value of 'nvidia.com/gpu.deploy.container-toolkit=${TOOLKIT_DEPLOYED}'"

    echo "Getting current value of the 'nvidia.com/gpu.deploy.device-plugin' node label"
    PLUGIN_DEPLOYED=$(kubectl get nodes ${NODE_NAME} -o=jsonpath='{$.metadata.labels.nvidia\.com/gpu\.deploy\.device-plugin}')
    if [ "${?}" != "0" ]; then
        echo "Unable to get the value of the 'nvidia.com/gpu.deploy.device-plugin' label"
        exit 1
    fi
    echo "Current value of 'nvidia.com/gpu.deploy.device-plugin=${PLUGIN_DEPLOYED}'"

    echo "Getting current value of the 'nvidia.com/gpu.deploy.gpu-feature-discovery' node label"
    GFD_DEPLOYED=$(kubectl get nodes ${NODE_NAME} -o=jsonpath='{$.metadata.labels.nvidia\.com/gpu\.deploy\.gpu-feature-discovery}')
    if [ "${?}" != "0" ]; then
        echo "Unable to get the value of the 'nvidia.com/gpu.deploy.gpu-feature-discovery' label"
        exit 1
    fi
    echo "Current value of 'nvidia.com/gpu.deploy.gpu-feature-discovery=${GFD_DEPLOYED}'"

    echo "Getting current value of the 'nvidia.com/gpu.deploy.dcgm-exporter' node label"
    DCGM_EXPORTER_DEPLOYED=$(kubectl get nodes ${NODE_NAME} -o=jsonpath='{$.metadata.labels.nvidia\.com/gpu\.deploy\.dcgm-exporter}')
    if [ "${?}" != "0" ]; then
        echo "Unable to get the value of the 'nvidia.com/gpu.deploy.dcgm-exporter' label"
        exit 1
    fi
    echo "Current value of 'nvidia.com/gpu.deploy.dcgm-exporter=${DCGM_EXPORTER_DEPLOYED}'"

    echo "Getting current value of the 'nvidia.com/gpu.deploy.dcgm' node label"
    DCGM_DEPLOYED=$(kubectl get nodes ${NODE_NAME} -o=jsonpath='{$.metadata.labels.nvidia\.com/gpu\.deploy\.dcgm}')
    if [ "${?}" != "0" ]; then
        echo "Unable to get the value of the 'nvidia.com/gpu.deploy.dcgm' label"
        exit 1
    fi
    echo "Current value of 'nvidia.com/gpu.deploy.dcgm=${DCGM_DEPLOYED}'"

    echo "Getting current value of the 'nvidia.com/gpu.deploy.mig-manager' node label"
    MIG_MANAGER_DEPLOYED=$(kubectl get nodes ${NODE_NAME} -o=jsonpath='{$.metadata.labels.nvidia\.com/gpu\.deploy\.mig-manager}')
    if [ "${?}" != "0" ]; then
        echo "Unable to get the value of the 'nvidia.com/gpu.deploy.mig-manager' label"
        exit 1
    fi
    echo "Current value of 'nvidia.com/gpu.deploy.mig-manager=${MIG_MANAGER_DEPLOYED}'"

}

_evict_gpu_operator_components() {
    echo "Shutting down all GPU clients on the current node by disabling their component-specific nodeSelector labels"
    kubectl label --overwrite \
        node ${NODE_NAME} \
        nvidia.com/gpu.deploy.operator-validator=$(_maybe_set_paused ${VALIDATOR_DEPLOYED}) \
        nvidia.com/gpu.deploy.container-toolkit=$(_maybe_set_paused ${TOOLKIT_DEPLOYED}) \
        nvidia.com/gpu.deploy.device-plugin=$(_maybe_set_paused ${PLUGIN_DEPLOYED}) \
        nvidia.com/gpu.deploy.gpu-feature-discovery=$(_maybe_set_paused ${GFD_DEPLOYED}) \
        nvidia.com/gpu.deploy.dcgm-exporter=$(_maybe_set_paused ${DCGM_EXPORTER_DEPLOYED}) \
        nvidia.com/gpu.deploy.dcgm=$(_maybe_set_paused ${DCGM_DEPLOYED})

    if [ "$?" != "0" ]; then
        return 1
    fi

    if [ "${MIG_MANAGER_DEPLOYED}" != "" ]; then
        kubectl label --overwrite \
            node ${NODE_NAME} \
            nvidia.com/gpu.deploy.mig-manager=$(_maybe_set_paused ${MIG_MANAGER_DEPLOYED})
        if [ "$?" != "0" ]; then
            return 1
        fi
    fi

    echo "Waiting for the operator-validator to shutdown"
    kubectl wait --for=delete pod \
        --timeout=5m \
        --field-selector "spec.nodeName=${NODE_NAME}" \
        -n ${OPERATOR_NAMESPACE} \
        -l app=nvidia-operator-validator

    echo "Waiting for the container-toolkit to shutdown"
    kubectl wait --for=delete pod \
        --timeout=5m \
        --field-selector "spec.nodeName=${NODE_NAME}" \
        -n ${OPERATOR_NAMESPACE} \
        -l app=nvidia-container-toolkit-daemonset

    echo "Waiting for the device-plugin to shutdown"
    kubectl wait --for=delete pod \
        --timeout=5m \
        --field-selector "spec.nodeName=${NODE_NAME}" \
        -n ${OPERATOR_NAMESPACE} \
        -l app=nvidia-device-plugin-daemonset

    echo "Waiting for gpu-feature-discovery to shutdown"
    kubectl wait --for=delete pod \
        --timeout=5m \
        --field-selector "spec.nodeName=${NODE_NAME}" \
        -n ${OPERATOR_NAMESPACE} \
        -l app=gpu-feature-discovery

    echo "Waiting for dcgm-exporter to shutdown"
    kubectl wait --for=delete pod \
        --timeout=5m \
        --field-selector "spec.nodeName=${NODE_NAME}" \
        -n ${OPERATOR_NAMESPACE} \
        -l app=nvidia-dcgm-exporter

    echo "Waiting for dcgm to shutdown"
    kubectl wait --for=delete pod \
        --timeout=5m \
        --field-selector "spec.nodeName=${NODE_NAME}" \
        -n ${OPERATOR_NAMESPACE} \
        -l app=nvidia-dcgm

    if [ "${MIG_MANAGER_DEPLOYED}" != "" ]; then
        echo "Waiting for mig-manager to shutdown"
        kubectl wait --for=delete pod \
            --timeout=5m \
            --field-selector "spec.nodeName=${NODE_NAME}" \
            -n ${OPERATOR_NAMESPACE} \
            -l app=nvidia-mig-manager
    fi

    return 0
}

_uncordon_k8s_node() {
    echo "Uncordoning node ${NODE_NAME}..."
    kubectl uncordon ${NODE_NAME}
    if [ "$?" != "0" ]; then
        return 1
    fi
    return 0
}

_reschedule_gpu_operator_components() {
    echo "Rescheduling all GPU clients on the current node by enabling their component-specific nodeSelector labels"
    kubectl label --overwrite \
        node ${NODE_NAME} \
        nvidia.com/gpu.deploy.operator-validator=$(_maybe_set_true ${VALIDATOR_DEPLOYED}) \
        nvidia.com/gpu.deploy.container-toolkit=$(_maybe_set_true ${TOOLKIT_DEPLOYED}) \
        nvidia.com/gpu.deploy.device-plugin=$(_maybe_set_true ${PLUGIN_DEPLOYED}) \
        nvidia.com/gpu.deploy.gpu-feature-discovery=$(_maybe_set_true ${GFD_DEPLOYED}) \
        nvidia.com/gpu.deploy.dcgm-exporter=$(_maybe_set_true ${DCGM_EXPORTER_DEPLOYED}) \
        nvidia.com/gpu.deploy.dcgm=$(_maybe_set_true ${DCGM_DEPLOYED})

    if [ "$?" != "0" ]; then
        return 1
    fi

    if [ "${MIG_MANAGER_DEPLOYED}" != "" ]; then
        kubectl label --overwrite \
            node ${NODE_NAME} \
            nvidia.com/gpu.deploy.mig-manager=$(_maybe_set_true ${MIG_MANAGER_DEPLOYED})
        if [ "$?" != "0" ]; then
            return 1
        fi
    fi
    return 0
}

_driver_busy() {
    local nvidia_refs=0
    if [ -f /sys/module/nvidia/refcnt ]; then
        nvidia_refs=$(< /sys/module/nvidia/refcnt)
    fi
    if [ ${nvidia_refs} -gt 0 ]; then
        echo "nvidia driver module is already loaded with refcount ${nvidia_refs}"
        return 0
    fi
    return 1
}

_nouveau_loaded() {
    if [ -f /sys/module/nouveau/refcnt ]; then
        return 0
    fi
    return 1
}

_unload_nouveau() {
    if [ -f /sys/module/nouveau/refcnt ]; then
        echo "Unloading nouveau driver..."
        rmmod nouveau
        if [ "$?" != "0" ]; then
            echo "Failed to unload nouveau driver"
            return 1
        fi
    fi
    return 0
}

_exit_failed() {
    # below commands are no-op if node is already in desired state
    _uncordon_k8s_node
    _reschedule_gpu_operator_components
    exit 1
}

_exit_success() {
    # below commands are no-op if node is already in desired state
    _uncordon_k8s_node
    _reschedule_gpu_operator_components
    exit 0
}

uninstall_driver() {
    # fetch current status of all component labels
    _fetch_current_labels

    # check if driver is already loaded
    if _driver_busy; then
        # check if we can cleanup driver modules by just shutting down gpu-operator pods
        _evict_gpu_operator_components || _exit_failed
        _cleanup_driver
        if [ "$?" != "0" ]; then
            if _is_auto_drain_enabled; then
                echo "Unable to cleanup driver modules, attempting again with node drain..."
                trap '_exit_failed' ERR
                _drain_k8s_node
                _cleanup_driver
            else
                echo "Failed to uninstall nvidia driver components"
                _exit_failed
            fi
        fi
        echo "Successfully uninstalled nvidia driver components"
    fi

    if _is_auto_drain_enabled; then
        # uncordon the node in case if the pod has restarted abruptly after we cordoned the node
        _uncordon_k8s_node
    fi
    # always reschedule operator components in case if the pod has restarted abruptly after evicting pods
    _reschedule_gpu_operator_components

    # check for nouveau driver and unload it before driver nvidia installation
    if _nouveau_loaded; then
        _unload_nouveau
        if [ "$?" != "0" ]; then
            exit 1
        fi
        echo "Successfully unloaded nouveau driver"
    fi
    exit 0
}

preflight_check() {
    # TODO: add checks for driver package availability for current kernel
    # TODO: add checks for driver dependencies
    # TODO: add checks for entitlements(OCP)
    exit 0
}

# Unload the kernel modules if they are currently loaded.
_unload_driver() {
    local rmmod_args=()
    local nvidia_deps=0
    local nvidia_refs=0
    local nvidia_uvm_refs=0
    local nvidia_modeset_refs=0
    local nvidia_peermem_refs=0

    echo "Unloading NVIDIA driver kernel modules..."
    if [ -f /sys/module/nvidia_modeset/refcnt ]; then
        nvidia_modeset_refs=$(< /sys/module/nvidia_modeset/refcnt)
        rmmod_args+=("nvidia-modeset")
        ((++nvidia_deps))
    fi
    if [ -f /sys/module/nvidia_uvm/refcnt ]; then
        nvidia_uvm_refs=$(< /sys/module/nvidia_uvm/refcnt)
        rmmod_args+=("nvidia-uvm")
        ((++nvidia_deps))
    fi
    if [ -f /sys/module/nvidia_peermem/refcnt ]; then
        nvidia_peermem_refs=$(< /sys/module/nvidia_peermem/refcnt)
        rmmod_args+=("nvidia-peermem")
        ((++nvidia_deps))
    fi
    if [ -f /sys/module/nvidia/refcnt ]; then
        nvidia_refs=$(< /sys/module/nvidia/refcnt)
        rmmod_args+=("nvidia")
    fi
    if [ ${nvidia_refs} -gt ${nvidia_deps} ] || [ ${nvidia_uvm_refs} -gt 0 ] || [ ${nvidia_modeset_refs} -gt 0 ] || [ ${nvidia_peermem_refs} -gt 0 ]; then
        # run lsmod to debug module usage
        lsmod | grep nvidia
        echo "Could not unload NVIDIA driver kernel modules, driver is in use" >&2
        return 1
    fi

    if [ ${#rmmod_args[@]} -gt 0 ]; then
        rmmod ${rmmod_args[@]}
        if [ "$?" != "0" ]; then
            return 1
        fi
    fi
    return 0
}

# Unmount the driver rootfs from the run directory.
_unmount_rootfs() {
    echo "Unmounting NVIDIA driver rootfs..."
    if findmnt -r -o TARGET | grep "/run/nvidia/driver" > /dev/null; then
        umount -l -R /run/nvidia/driver
        return 0
    fi
    return 1
}

_cleanup_driver() {
    _unload_driver
    if [ "$?" != "0" ]; then
        return 1
    fi
    _unmount_rootfs
    if [ "$?" != "0" ]; then
        return 1
    fi
    if [ -f ${DRIVER_PID_FILE} ]; then
        rm -f ${DRIVER_PID_FILE}
    fi
    return 0
}

usage() {
    cat >&2 <<EOF
Usage: $0 COMMAND [ARG...]

Commands:
  uninstall_driver
  preflight_check
EOF
    exit 1
}

if [ $# -eq 0 ]; then
    usage
fi
command=$1; shift
case "${command}" in
    uninstall_driver) ;;
    preflight_check) ;;
    *) usage ;;
esac
if [ $? -ne 0 ]; then
    usage
fi

$command
